{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45fd6bb9-9cab-47fa-93df-0f3a650b0a2e",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "![Logistic Regression](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*CYAn9ACXrWX3IneHSoMVOQ.gif)\n",
    "\n",
    "In statistics logistic regression is used to model the probability of a certain class or event.\n",
    "\n",
    "Logistic regression is similar to linear regression because both of these involve estimating the values of parameters used in the prediction equation based on the given training data. Linear regression predicts the value of some continuous, dependent variable. Whereas logistic regression predicts the probability of an event or class that is dependent on other factors. Thus the output of logistic regression always lies between 0 and 1. Because of this property it is commonly used for classification purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669c3f0-aab7-48c9-ab31-b398d486ad1f",
   "metadata": {},
   "source": [
    "## Logistic Model\n",
    "Consider a model with features *x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>¬†‚Ä¶ x<sub>n</sub>*. Let the binary output be denoted by *Y*, that can take the values 0 or 1.  \n",
    "Let *p* be the probability of *Y = 1*, we can denote it as *p = P(Y=1)*.  \n",
    "The mathematical relationship between these variables can be denoted as:  \n",
    "\n",
    "$$ ln(\\frac{p}{1-p}) = b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n $$\n",
    "\n",
    "Here the term $\\frac{p}{1-p}$ is known as the *odds* and denotes the likelihood of the event taking place. Thus $ln(\\frac{p}{1-p})$ is known as the *log odds* and is simply used to map the probabilty that lies between 0 and 1 to a range between $(-\\infty, +\\infty)$. The terms $b_0, b_1, b_2 ...$ are parameters (or weights) that we will estimate during training.   \n",
    "\n",
    "So this is just the basic math behind what we are going to do. We are interested in the probability *p* in this equation. So we simplify the equation to obtain the value of p:  \n",
    "1. The log term $ln$ on the LHS can be removed by raising the RHS as a power of $e$:  \n",
    "$$ \\frac{p}{1-p}  = e^{b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n} $$\n",
    "2. Now we can easily simplify to obtain the value of $p$:  \n",
    "$$ p = \\frac{e^{b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n}}{1 + e^{b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n}} $$\n",
    "$$ or $$\n",
    "$$ p = \\frac{1}{1 + e^{-(b_0 + b_1x_1 + b_2x_2 + b_3x_3 ... b_nx_n)}} $$   \n",
    "\n",
    "This actually turns out to be the equation of the *Sigmoid Function* which is widely used in other machine learning applications. The *Sigmoid Function* is given by:\n",
    "$$ S(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "Now we will be using the above derived equation to make our predictions. Before that we will train our model to obtain the values of our parameters $b_0, b_1, b_2 ...$ that result in least error. This is where the error or loss function comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876a1c70-4261-4b45-92b3-7b785b350fdf",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "Logistic Regression uses a loss function more suited to the task of categorization where the target is 0 or 1 rather than any number. \n",
    "\n",
    ">**Definition Note:**   In this course, these definitions are used:  \n",
    "**Loss** is a measure of the difference of a single example to its target value while the  \n",
    "**Cost** is a measure of the losses over the training set\n",
    "\n",
    "\n",
    "This is defined: \n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "\\begin{equation}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "    - \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "    - \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "  \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$ is the target value.\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$ where function $g$ is the sigmoid function.\n",
    "\n",
    "The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target. Consider the curves below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962bcd6f-f437-47a9-ad94-9cae74413fec",
   "metadata": {},
   "source": [
    "The loss function above can be rewritten to be easier to implement.\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "  \n",
    "This is a rather formidable-looking equation. It is less daunting when you consider $y^{(i)}$ can have only two values, 0 and 1. One can then consider the equation in two pieces:  \n",
    "when $ y^{(i)} = 0$, the left-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "and when $ y^{(i)} = 1$, the right-hand term is eliminated:\n",
    "$$\n",
    "\\begin{align}\n",
    "  loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "  &=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "OK, with this new logistic loss function, a cost function can be produced that incorporates the loss from all the examples. This will be the topic of the next lab. For now, let's take a look at the cost vs parameters curve for the simple example we considered above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599dcde-7f11-456a-af6b-9d1f2dde5f6e",
   "metadata": {},
   "source": [
    "In a previous lab, you developed the *logistic loss* function. Recall, loss is defined to apply to one example. Here you combine the losses to form the **cost**, which includes all the examples.\n",
    "\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "*  where m is the number of training examples in the data set and:\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcaedac-245a-4126-8b4b-4f78a66d969a",
   "metadata": {},
   "source": [
    "<a name='ex-02'></a>\n",
    "#### Code Description\n",
    "\n",
    "The algorithm for `compute_cost_logistic` loops over all the examples calculating the loss for each example and accumulating the total.\n",
    "\n",
    "Note that the variables X and y are not scalar values but matrices of shape ($m, n$) and ($ùëö$,) respectively, where  $ùëõ$ is the number of features and $ùëö$ is the number of training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9aac50-1f4d-4933-9cd3-5e8092f6ca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c0d4733-57e0-4269-80c6-ec59f27a2ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b, *argv):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      *argv : unused, for compatibility with regularized version below\n",
    "    Returns:\n",
    "      total_cost : (scalar) cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "     \n",
    "    ### START CODE HERE ###\n",
    "    loss_sum = 0 \n",
    "\n",
    "    # Loop over each training example\n",
    "    for i in range(m): \n",
    "\n",
    "        # First calculate z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b\n",
    "        z_wb = 0 \n",
    "        # Loop over each feature\n",
    "        for j in range(n): \n",
    "            # Add the corresponding term to z_wb\n",
    "            z_wb_ij = w[j]*X[i][j]\n",
    "            z_wb += z_wb_ij # equivalent to z_wb = z_wb + z_wb_ij\n",
    "        # Add the bias term to z_wb\n",
    "        z_wb += b # equivalent to z_wb = z_wb + b\n",
    "\n",
    "        f_wb = sigmoid(z_wb)\n",
    "        loss =  -y[i] * np.log(f_wb) - (1 - y[i]) * np.log(1 - f_wb)\n",
    "\n",
    "        loss_sum += loss # equivalent to loss_sum = loss_sum + loss\n",
    "\n",
    "    total_cost = (1 / m) * loss_sum  \n",
    "            \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "\n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f449c938-6eec-4aac-a04d-4f47db18ae01",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m m, n \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Compute and display cost with w and b initialized to zeros\u001b[39;00m\n\u001b[0;32m      4\u001b[0m initial_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "m, n = X_train.shape\n",
    "\n",
    "# Compute and display cost with w and b initialized to zeros\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print('Cost at initial w and b (zeros): {:.3f}'.format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a37bb-534c-43b2-aeaa-7c42ff0048fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display cost with non-zero w and b\n",
    "test_w = np.array([0.2, 0.2])\n",
    "test_b = -24.\n",
    "cost = compute_cost(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Cost at test w and b (non-zeros): {:.3f}'.format(cost))\n",
    "\n",
    "\n",
    "# UNIT TESTS\n",
    "compute_cost_test(compute_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55dd21-9df2-48dd-be6f-77013c33b5df",
   "metadata": {},
   "source": [
    "\n",
    "## The Gradient Descent Algorithm\n",
    "You might know that the partial derivative of a function at it's minimum value is equal to 0. So gradient descent basically uses this concept to estimate the parameters or weights of our model by minimizing the loss function. [Click here](https://www.youtube.com/watch?v=4PHI11lX11I) for a more detailed explanation on how gradient descent works.  \n",
    "For simplicity, for the rest of this tutorial let us assume that our ouptput depends only on a single feature $x$. So we can rewrite our equation as:\n",
    "$$ \\bar{y_i} = p = \\frac{1}{1 + e^{-(b_0 + b_1x_i)}} = \\frac{1}{1 + e^{-b_0 -b_1x_i}} $$  \n",
    "Thus we need to estimate the values of weights $b_0$ and $b_1$ using our given training data.\n",
    "1. Initially let $b_0=0$ and $b_1=0$. Let $L$ be the learning rate. The learning rate controls by how much the values of $b_0$ and $b_1$ are updated at each step in the learning process. Here let $L = 0.001$.\n",
    "2. Calculate the partial derivative with respect to $b_0$ and $b_1$. The value of the partial derivative will tell us how far the loss function is from it's minimum value. It is a measure of how much our weights need to be updated to attain minimum or ideally 0 error. In case you have more than one feature, you need to calculate the partial derivative for each weight $b_0$, $b_1$ ... $b_n$ where $n$ is the number of features. For a detailed explanation on the math behind calculating the partial derivatives, check out [my video.](https://youtu.be/l8VEth6leXA)\n",
    "$$ D_{b_0} = -2 \\sum_{i=1}^n(y_i - \\bar{y_i}) \\times \\bar{y_i} \\times (1 - \\bar{y_i}) $$\n",
    "$$ D_{b_1} = -2 \\sum_{i=1}^n(y_i - \\bar{y_i}) \\times \\bar{y_i} \\times (1 - \\bar{y_i}) \\times x_i $$\n",
    "3. Next we update the values of $b_0$ and $b_1$:\n",
    "$$ b_0 = b_0 - L \\times D_{b_0} $$\n",
    "$$ b_1 = b_1 - L \\times D_{b_1} $$  \n",
    "4. We repeat this process until our loss function is a very small value or ideally reaches 0 (meaning no errors and 100% accuracy). The number of times we repeat this learning process is known as iterations or epochs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "523de087-0471-4ece-8271-5871657a12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_gradient_logistic(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "    Returns\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)      : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                           #(n,)\n",
    "    dj_db = 0.\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "        \n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e62ffb70-7ba2-4306-8290-93ec6d2838b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b1909c6-d421-42a6-88c8-1de1fbc9c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516844d0-02e8-4d95-8854-9f3396c4bfe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./breast-cancer.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e161d93-71d6-4abc-8487-ca496ed3a8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
